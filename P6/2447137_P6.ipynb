{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "glKoxaE3LjBz"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "XiCLZikjLlHj"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "# Path to the dataset downloaded from Kaggle\n",
        "MAX_VOCAB_SIZE = 500  # Limit vocabulary to speed up training for lab purposes\n",
        "MAX_SEQUENCE_LEN = 20  # Length of sequence window (sliding window size)\n",
        "EPOCHS = 10\n",
        "EMBEDDING_DIM = 50\n",
        "LSTM_UNITS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5eoItUPLuld",
        "outputId": "25ab6f0b-57a9-4dda-8f81-f7e26d09aeab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from PoetryFoundationData.csv...\n",
            "Loaded 1000 poems\n",
            "Average poem length: 1760 characters\n"
          ]
        }
      ],
      "source": [
        "def load_data_improved():\n",
        "    \"\"\"\n",
        "    Enhanced data loading with better preprocessing for LSTM training\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Direct path to CSV\n",
        "        csv_path = 'PoetryFoundationData.csv'\n",
        "        \n",
        "        print(f\"Loading dataset from {csv_path}...\")\n",
        "        df = pd.read_csv(csv_path, encoding='utf-8', on_bad_lines='skip')\n",
        "        \n",
        "        # The Poetry Foundation dataset has a 'Poem' column\n",
        "        if 'Poem' in df.columns:\n",
        "            # Filter out very short poems (less than 50 characters)\n",
        "            df = df[df['Poem'].str.len() > 50]\n",
        "            \n",
        "            # Take a larger subset for better training (1000-2000 poems)\n",
        "            corpus = df['Poem'].astype(str).dropna().tolist()[:1000]\n",
        "            \n",
        "            # Optional: Clean the text\n",
        "            corpus = [poem.lower().strip() for poem in corpus]\n",
        "            \n",
        "            print(f\"Loaded {len(corpus)} poems\")\n",
        "            print(f\"Average poem length: {np.mean([len(p) for p in corpus]):.0f} characters\")\n",
        "            \n",
        "            return corpus\n",
        "        else:\n",
        "            raise ValueError(\"'Poem' column not found in dataset\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "corpus_raw = load_data_improved()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMezAwCfLyjx",
        "outputId": "00ee2212-7474-4b3c-8049-3d3efa47daed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing data...\n",
            "Total unique words: 35176\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA PREPROCESSING\n",
        "# ==========================================\n",
        "print(\"Preprocessing data...\")\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
        "tokenizer.fit_on_texts(corpus_raw)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(f\"Total unique words: {total_words}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebyu-RVzL1a5",
        "outputId": "b20d61aa-aad8-42bc-a95f-0ba110b15352"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Shape: (199734, 19)\n",
            "Output Shape: (199734,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create Input Sequences (Sliding Window)\n",
        "input_sequences = []\n",
        "for line in corpus_raw:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad Sequences\n",
        "# We pad 'pre' (before) so the model learns from the end of the sequence\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=MAX_SEQUENCE_LEN, padding='pre'))\n",
        "\n",
        "# Create Predictors (X) and Label (y)\n",
        "# X is everything up to the last word, y is the last word\n",
        "xs, labels = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "\n",
        "# One-hot encode the labels (Categorical Crossentropy requirement)\n",
        "# Keep labels as integers (no one-hot encoding needed)\n",
        "ys = labels  # Just use the integer labels directly\n",
        "\n",
        "print(f\"Input Shape: {xs.shape}\")\n",
        "print(f\"Output Shape: {ys.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "Yh3t8E9fL3k9",
        "outputId": "2696c3aa-496a-49bf-f37a-aa6b30caafe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Building Model...\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 19, 50)            1758800   \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 100)               60400     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 35176)             3552776   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5371976 (20.49 MB)\n",
            "Trainable params: 5371976 (20.49 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==========================================\n",
        "# 3. LSTM MODEL DEVELOPMENT\n",
        "# ==========================================\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input # Add Input here\n",
        "\n",
        "print(\"\\nBuilding Model...\")\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding Layer: Input Dim = Vocab Size, Output Dim = 100\n",
        "# model.add(Embedding(total_words, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LEN-1))\n",
        "# # NEW WAY (Fixes Warning)\n",
        "model.add(Input(shape=(MAX_SEQUENCE_LEN-1,))) # Add this line\n",
        "model.add(Embedding(total_words, EMBEDDING_DIM)) # Remove input_length here\n",
        "\n",
        "# LSTM Layer: 100 Units\n",
        "model.add(LSTM(LSTM_UNITS))\n",
        "\n",
        "# Dropout Layer: 0.2 to prevent overfitting\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Dense Output Layer: Softmax activation for word prediction\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kviW_h8VL5rN",
        "outputId": "17bfbe29-3e36-41dc-81b3-659825245347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Compiling and Training...\n",
            "6242/6242 [==============================] - 438s 70ms/step - loss: 4.7316 - accuracy: 0.1260\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==========================================\n",
        "# 4. TRAINING \n",
        "# ==========================================\n",
        "print(\"\\nCompiling and Training...\")\n",
        "# Use sparse_categorical_crossentropy instead of categorical_crossentropy\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(xs, ys, epochs=1, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "COR4DVZzLhHb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ==========================================\n",
        "# 5. TEXT GENERATION\n",
        "# ==========================================\n",
        "def generate_poem(seed_text, next_words, model, max_sequence_len):\n",
        "    \"\"\"\n",
        "    Generates new text based on a seed text.\n",
        "    \"\"\"\n",
        "    generated_text = seed_text\n",
        "\n",
        "    print(f\"\\n--- Generating {next_words} words starting with: '{seed_text}' ---\")\n",
        "\n",
        "    for _ in range(next_words):\n",
        "        # Tokenize the current text\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "        # Pad the sequence (must match training padding)\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "        # Predict probabilities\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "\n",
        "        # Get the class with highest probability\n",
        "        predicted_index = np.argmax(predicted_probs, axis=-1)[0]\n",
        "\n",
        "        # Convert index back to word\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # Append word to seed_text for next iteration\n",
        "        seed_text += \" \" + output_word\n",
        "        generated_text += \" \" + output_word\n",
        "\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CgtlTsmL7hg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Generated Poem 1 ---\n",
            "\n",
            "--- Generating 10 words starting with: 'The woods' ---\n",
            "The woods of the of the of the of the of the\n",
            "\n",
            "--- Generated Poem 2 ---\n",
            "\n",
            "--- Generating 8 words starting with: 'And sorry I' ---\n",
            "And sorry I have to the of the of the of\n",
            "\n",
            "--- Generated Poem 3 (Custom) ---\n",
            "\n",
            "--- Generating 12 words starting with: 'I shall be' ---\n",
            "  shall be \n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"--- Generated Poem 1 ---\")\n",
        "print(generate_poem(\"The woods\", 10, model, MAX_SEQUENCE_LEN))\n",
        "\n",
        "print(\"\\n--- Generated Poem 2 ---\")\n",
        "print(generate_poem(\"And sorry I\", 8, model, MAX_SEQUENCE_LEN))\n",
        "\n",
        "print(\"\\n--- Generated Poem 3 (Custom) ---\")\n",
        "print(generate_poem(\"I shall be\", 12, model, MAX_SEQUENCE_LEN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnVwg-ZsMYNv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
